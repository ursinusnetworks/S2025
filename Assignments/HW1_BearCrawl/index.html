<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 475: Computer Networks, Spring 2025</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 475: Computer Networks, Spring 2025</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Homework 1: "Bear Crawl" Web Crawler (25 Points)</h2>
										<h3><a href = "https://www.ctralie.com">Chris Tralie</a></h3>
									</header>

									<div id="page-content">

										<img src = "bearcrawl.png" width="80%">
										<p>
											Bear image CC0 creative commons image courtesy of <a href = "https://www.rawpixel.com/image/4031542/cub-scavenging">rawpixel.com</a>
										</p>

                                        <p>
                                            <h3><a name = "objectives">Learning Objectives</a></h3>
                                            <ul>
												<li>Conceptualize a system of links as a graph data structure</li>
												<li>Implement breadth first search in a fun application</li>
												<li>Carefully read documentation about web scraping libraries</li>
                                            </ul>
										</p>
										

										<h3><a name = "overview">Description / Overview</a></h3>
										<p>
											In this assignment, you will create a <a href = "https://ursinusdatastructures.github.io/Modules/BFSDFSTSP/Video1">breadth-first</a> web crawler in python to discover all of the HTML pages on a web site, starting at some base url, up to some number of clicks.  For instance, the <a href = "#viewer">viewer below</a> shows all HTML pages that are within two clicks of <a href = "https://www.ursinus.edu">https://www.ursinus.edu</a>.  Since we're the Ursinus bears, we may as well call this the "bear crawler" in that case üêª.  But the code will be general enough to work on any site, and students are encouraged to try this on other web sites and share results on the class discord!
										</p>
										<p>
											This assignment starts us up at the very top of the application level at the top of the network stack.  In subsequent assignments, we'll pull ourselves down to lower parts of the application layer and implement parts of the HTTP protocol directly in C, but for now, students will use a higher level implementation of this protocol in python's <a href = "https://docs.python.org/3/library/urllib.html">urllib</a> library so they can focus on how to use the result of HTTP communication in a cool application.
										</p>

										<h3><a name = "ethics">Ethics of Web Crawling</a></h3>
										<p>
											One theme throughout the course is that with great power comes great responsibility.  On the very day that I released this assignment (1/23/2025), there was an <a href = "https://www.404media.co/email/6558718f-dbe9-47a0-a57f-5e80e0ed44ce/">article on 404 media</a> about how some AI companies are abusing grossly inefficient web crawlers to create their training data, and some creative solutions that web site owners have taken to combat this by creating infinite links on web sites (so-called "spider traps").  Any web crawler that is minimally compliant to a web host's wishes will read and respect the <a href = "https://developers.google.com/search/docs/crawling-indexing/robots/intro">robots.txt</a> file that specifies constraints on what a crawler should do.  We won't do this since we're sticking to ursinus.edu, whose <a href = "https://www.ursinus.edu/robots.txt">robots.txt file</a> simply says that crawlers shouldn't follow any "reply to" links to avoid going into infinitely deep traversals.  But we're doing a breadth-first search only up to a certain amount, so we'll never have to worry about this problem.  And we're also being reasonably courteous by <a href = "#sleep">sleeping in between requests</a>.  But be mindful of this if you use your code on this assignment in other contexts, and <b>be a good internet citizen!</b>
										</p>

										<h3><a name = "gettingstarted">Getting Started / What To Submit</a></h3>
										<p>
											Take a moment now to make sure that python is setup on your computer (you may want to look back at the <a href = "https://ursinusdatastructures.github.io/F2024/Software/index.html">data structures page about this</a>).
										</p>
										<p>
											There is no starter code for this assignment.  Instead, students should create a Jupyter notebook called <code>crawler.ipynb</code> or a python file called <code>crawler.py</code>, which they'll hand in on Canvas at the end of the assignment.  In addition to using the standard python library <a href = "https://docs.python.org/3/library/urllib.html">urllib</a> to help with parsing URLs and loading HTML, I'd recommend that you use the <a href = "https://pypi.org/project/beautifulsoup4/">Beautiful Soup</a> library to find all of the links on a page.  If you're having trouble installing this library on your computer, you can use a Jupyter notebook in <a href = "https://colab.research.google.com/">Google colab</a>.
										</p>
										
										


										<HR>
										<h2><a name = "programming">Programming Tasks</a></h2>

										<p>
											If we consider a web page as a node in a graph, and we put an edge between two pages if there exists a link from one to the other, then a web crawler is just a glorified graph search algorithm.  But the devil is in the details.  First, in <a href = "#urlparse">part 1</a>, we have to do a fair bit of work to clean up the URLs so we don't accidentally mistake two versions of the same URL as two different pages.  There is no perfect method of doing this, but we'll make some informed choices make a reasonably clean web crawler.
										<p>
											Then, before creating the crawler in earnest in <a href = "#crawl">part 2</a>, you should review the <a href = "https://ursinusdatastructures.github.io/Modules/BFSDFSTSP/Video1">breadth first search notes from data structures</a>.  You may also want to review my video on <a href = "https://www.youtube.com/watch?v=DDdYFmSXrm0">solving 8-puzzle with breadth-first search</a>.  You can borrow some of the code from these examples if you'd like, though you shouldn't necessarily need a graph class, since all I want you to do is return a list of edges, each of which is a pair of URLs that are connected by a link.  Also, you can python's built-in <a href = "https://docs.python.org/3/library/collections.html#collections.deque">deque</a> instead of our custom doubly-linked list implementation from that class.
											</p>
										</p>

										<h3><a name = "urlparse">Part 1: Neighbor URL Parsing (7 Points)</a></h3>
										<p>
											Suppose we're at a web page <a href = "https://www.ursinus.edu/offices/registrar/meet-the-staff">https://www.ursinus.edu/offices/registrar/meet-the-staff</a> and we see a link tag that looks like this:
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[

											<a href = "../academic-calendar">Link to academic calendar</a>
											</script> 

										<p>
											The <code>href</code> part of the tag gives the link we want to follow to a neighboring page, but it is a <i>relative path</i>, not an absolute path.  This is the norm with web sites to make them more portable.  But we need to convert it into an absolute URL before we can visit it.  Thankfully, the <code>urljoin</code> method of the <a href = "https://docs.python.org/3/library/urllib.parse.html">urllib.parse</a> library gets us most of the way there.  For instance:
										</p>

										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											from urllib.parse import urljoin
											urljoin("https://www.ursinus.edu/offices/registrar/meet-the-staff", "../academic-calendar")
											</script> 
										<p>
											Will give us the link <a href = "https://www.ursinus.edu/offices/academic-calendar">https://www.ursinus.edu/offices/academic-calendar</a>.  However, there is some more work we need to do to prepare our links for a good web crawler.
										</p>

										<h4>Your Task</h4>
										<p>
											Create a method <code>next_path(base, href)</code> that takes two parameters:
											<ul>
												<li><code>base</code>: The URL of the current page</li>
												<li><code>href</code>: The value of the <code>href</code> property of an <code>a</code> tag in the current page that links to another page page</li>
											</ul>
										</p>
										<p>
											Your method should return a cleaned up string corresponding to the absolute URL of the neighboring page.  First, do the following to <code>base</code> and <code>url</code>:

											<ul>
												<li>
													If <code>base</code> doesn't end in a file, add a "/" to it
												</li>
												<li>
													If <code>href</code> starts with <code>www.</code>, add <code>http://</code> to the beginning of it
												</li>
											</ul>
										</p>
										<p>
											Then, join <code>base</code> and <code>href</code> using <a href = "https://docs.python.org/3/library/urllib.parse.html">urllib.parse.urljoin</a>, and do the following modifications:
										</p>
										<ul>
											<li>
												If the URL ends in <code>index.*</code>, remove the end of it.  For example, this will let us realize that the page <a href = "https://www.ursinus.edu">https://www.ursinus.edu</a> is the same as <a href = "https://www.ursinus.edu/index.php">https://www.ursinus.edu/index.php</a>
											</li>
											<li>
												If there is at least one <code>#</code> character, remove the first <code>#</code> and everything following it.  This is so that a page with an "anchor" isn't counted as a unique page (this is a subjective choice for a web crawler, but we'll stick with it in this assignment to cut down on the number of unique pages)
											</li>
											<li>
												Finally, if the URL ends in a <code>/</code>, remove the <code>/</code>.  For instance, we want to recognize that <a href = "https://www.ursinus.edu">https://www.ursinus.edu</a> and <a href = "https://www.ursinus.edu/">https://www.ursinus.edu/</a> are the same page.
											</li>
										</ul>

										<h4>Tests</h4>
										<p>
											If this is working properly, you should get the following outputs from your <code>next_path</code> method:
										</p>
										<table>
											<tr>
												<td>
													<h4>Input</h4>
												</td>
												<td>
													<h4>Output</h4>
												</td>
											</tr>
											<tr>
												<td>
													<code>next_path("https://www.ursinus.edu", "/")</code>
												</td>
												<td>
													<a href = "https://www.ursinus.edu">https://www.ursinus.edu</a>
												</td>
											</tr>
											<tr>
												<td>
													<code>next_path("https://www.ursinus.edu/offices/registrar", "../")</code>
												</td>
												<td>
													<a href = "https://www.ursinus.edu/offices">https://www.ursinus.edu/offices</a>
												</td>
											</tr>
											<tr>
												<td>
													<code>next_path("https://www.ursinus.edu/offices", "registrar/meet-the-staff/")</code>
												</td>
												<td>
													<a href = "https://www.ursinus.edu/offices/registrar/meet-the-staff">https://www.ursinus.edu/offices/registrar/meet-the-staff</a>
												</td>
											</tr>
											<tr>
												<td>
													<code>next_path("https://www.ursinus.edu/academics", "www.yahoo.com/CoolStuff/index.php")</code>
												</td>
												<td>
													<a href = "http://www.yahoo.com/CoolStuff">http://www.yahoo.com/CoolStuff</a>
												</td>
											</tr>
											<tr>
												<td>
													<code>next_path("https://www.ursinus.edu/offices/marketing-and-communications/web-accessibility-statement", "#main-content")</code>
												</td>
												<td>
													<a href = "https://www.ursinus.edu/offices/marketing-and-communications/web-accessibility-statement">https://www.ursinus.edu/offices/marketing-and-communications/web-accessibility-statement</a>
												</td>
											</tr>
											<tr>
												<td>
													<code>next_path("https://www.ursinus.edu", "live/files/5133-web-academiccalendar2024-25.pdf")</code>
												</td>
												<td>
													<a href = "https://www.ursinus.edu/live/files/5133-web-academiccalendar2024-25.pdf">https://www.ursinus.edu/live/files/5133-web-academiccalendar2024-25.pdf</a>
												</td>
											</tr>
											<tr>
												<td>
													<code>next_path("https://www.ctralie.com/Teaching/Tutorials.html", "PoissonImageEditing")</code>
												</td>
												<td>
													<a href = "https://www.ctralie.com/Teaching/PoissonImageEditing">https://www.ctralie.com/Teaching/PoissonImageEditing</a>
												</td>
											</tr>
										</table>


										<h3><a name = "craw">Part 2: BFS Crawler (18 Points)</a></h3>
										<p>
											Now we're ready to make the actual web crawler
										</p>

										<h4>Your Task</h4>
										<p>
											Create a method <code>crawl_page(base_url, max_clicks, filename)</code>.  The crawling should start at <code>base_url</code> and visit every page that can be accessed <b>on that domain</b> from <code>base_url</code> <b>within <code>max_clicks</code> clicks</b> by following a sequence of <code>href</code> values in <code>&lt;a&gt;</code> tags.  You can use <code><a href = "https://docs.python.org/3/library/urllib.request.html">urllib.request.urlopen</a></code> to open each URL and check its <code>Content-Type</code>, and then <code>read()</code>its data if its's the correct type.  You can then use <a href = "https://pypi.org/project/beautifulsoup4/">BeautifulSoup</a> to find all of the <code>&lt;a&gt;</code> with <code>href</code> fields in this document.
										</p>
										<p>
											Crucially, you should <b>not</b> visit pages that are on a different domain from <code>base_url</code></b>.  So, for example, if my <code>base_url</code> is <a href = "https://www.ursinus.edu">https://www.ursinus.edu</a> and I see a link to <a href = "https://www.ctralie.com/Teaching">https://www.ctralie.com/Teaching</a>, you should <b>not</b> visit that link.
										</p>
										<p>
											As you're crawling the website, create a list of <code>edges</code>, each of which is a list of two pages that are connected to each other by a link.  Then, every time you discover a new link, save this info to a JSON file:
										</p>
										<script type="syntaxhighlighter" class="brush: python"><![CDATA[
											json.dump({"base_url":base_url, "edges":list(edges)}, open(filename, "w"))
										</script> 
										<p>
											You can open this file up in <a href = "#viewer">graph viewer below</a> to check your progress.  As I mentioned before, <code>edges</code>, should be a list of lists of pairs of URLs that are connected by a link, in no particular order.  <a href = "ctralie2.json">Click here</a> for an example of the edges on <a href = "https://www.ctralie.com">https://www.ctralie.com</a> up to two clicks (note that self edges will happen since pages often refer to themselves on a navigation menu!).
										</p>
										

										<p>
											Additionally, you should adhere to the following requirements as you're going along:
										</p>
										<ol>
											<li>
												You should gracefully handle any errors without crashing.  For example, if you can't load a particular URL, simply don't load and expand it.  Have a look at <a href = "https://docs.python.org/3/tutorial/errors.html">python error handling</a> to see how to do this using <code>try/catch</code>
											</li>
											<li>
												You should not visit a URL more than once.  This is baked into the BFS algorithm, but be sure you're keeping proper track of the links you've visited somehow
											</li>
											<li>
												We're sticking to HTML pages in this assignment, so you should only read the data from a URL if the <code>Content-Type</code> in the header contains <code>"text/html"</code>.  This will prevent you from downloading things like videos or pdf files that will crash your code (not to mention wasting bandwidth)
											</li>
											<li>
												<a name = "sleep">Sleeping</a>
												<p>
													To be kind to the web site that you're crawling, particularly while you're debugging, be sure to sleep for at least one second between subsequent web page requests
												</p>
												<script type="syntaxhighlighter" class="brush: python"><![CDATA[
													from time import sleep
													#...
													sleep(1)
												</script> 
											</li>
										</ol>

										<p>
											<b>NOTE:</b> You might want to crawl <a href = "https://www.ctralie.com/">https://www.ctralie.com/</a> as you're developing before you crawl ursinus.edu.  It's a simpler web site, and I don't mind as much if you crawl me while you're debugging üòÖ
										</p>
										
										

										<HR>
										<h2><a name = "viewer">Graph Viewer</a></h2>

										<p>
											Below is a live applet that you can use to view graphs that result from crawling.  I have four default graphs that you can play with, but you should be sure to test the graphs you're creating as you're going along by uploading your generated JSON files.
										</p>
										
                                    


										<div id="ForceContainer">


											<table>
												<tr>
													<td><p>Choose File<input type = "file" id = "graphInput"></p>

														<div class = "select-wrapper">
															<select id="examples" name = "examples">
																<option disabled selected value> -- select an option -- </option>
																<option value = "ctralie2.json">https://www.ctralie.com, depth 2</option>
																<option value = "ctralie3.json">https://www.ctralie.com, depth 3</option>
																<option value = "ursinus2.json" selected="selected">https://www.ursinus.edu, depth 2</option>
																<option value = "ursinus3.json">https://www.ursinus.edu, depth 3</option>
															</select>
														</div>

													</td>
												</tr>
												<tr>
													<td>
														<table>
															<tr>
																<td>
																	<input type="button" value="Search" onclick="search()"/>
																</td>
																<td>
																	<input type="text" id="searchInput" size="30"/>
																</td>
															</tr>
														</table>
														

														
													</td>
												</tr>
												<tr>
													<td><p><h3>URL</h3></p>
														<p id="url">URL Will show here when you mouse over a node</p>
													</td>
												</tr>
									
											</table>
											
											<svg id="ForceCanvas"></svg>
									
											
											<h3>Force Directed Graph GUI Directions</h3>
											<ul>
												<li> CTRL+Click to pan the view</li>
												<li> CTRL+Scroll to zoom in and out</li>
												<li> Left click and drag a node to move it around, and double click on a node to visit the corresponding page</li>
											</ul>
									
										</div>
										<script src="d3.min.js"></script>
										<script src="jquery-3.5.1.min.js"></script>
										<script src = "canvas.js"></script>
									
										<script>
											let canvas = new ForceCanvas();
											const width = window.innerWidth*0.7;
											const height = window.innerHeight*0.7;

										
										
										graphInput.addEventListener('change', function(e) {
											let reader = new FileReader();
											reader.onload = function(e) {
												canvas.updateParams(
													{
														"width":width, 
														"height":height,
														"graph":JSON.parse(e.target.result)
													}
												);
											}
											reader.readAsText(graphInput.files[0]);
										});

										function loadPrecomputedExample(path) {
											$.get(path, function(src) {
												canvas.updateParams(
													{
														"width":width, 
														"height":height,
														"graph":src
													}
												);
											});
										}

										let exampleMenu = document.getElementById("examples");
										exampleMenu.addEventListener('change', function(e){
											loadPrecomputedExample(e.target.value);
										});

									
										function search() {
											canvas.search(document.getElementById("searchInput").value);
										}

										loadPrecomputedExample("ursinus2.json");
										</script>

                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#deliverables">Deliverables</a>
													<ul>
														<li><a href = "../../index.html#debugging">Debugging Principles</a></li>
													</ul>
												</li>
												<li><a href = "../../index.html#grading">Grading</a>
													<ul>
														<li><a href = "../../index.html#deadlines">Deadlines Policy</a></li>
													</ul>
												</li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#participation">Participation</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul>
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
										<li>
											<span class="opener">Assignments</span>
											<ul>
												<li><a href = "../../Assignments/HW0_Software/index.html">HW0: Software Test / Wireshark / Mininet Intro</a></li>
												<li><a href = "../../Assignments/HW1_BearCrawl/index.html">HW1: "Bear Crawl" Web Crawler</a></li>
												<li><a href = "../../Assignments/HW2_HTTPWebGet/index.html">HW2: HTTP WebGet</a></li>
												<li><a href = "../../Assignments/HW3_ProxyServer/index.html">HW3: Proxy Server / Web Cache</a></li>
												<li><a href = "../../Assignments/HW4_DNSQuery/index.html">HW4: DNS Queries in Python</a></li>
												<li><a href = "../../Assignments/HW5_Wireshark/index.html">HW5: Wireshark And The Network Layer</a></li>
												<li><a href = "../../Assignments/HW6_GeoRouteMap/index.html">HW6: ICMP And Geographic Mapping</a></li>
											</ul>
										</li>
										<li><a href = "https://docs.google.com/forms/d/e/1FAIpQLSfwkO_w_Ku-n2Ou6J7pF--i0C2-a20Ov9wf690T6cYx80ASsw/viewform?usp=sf_link">Anonymous Question</a></li>
									</ul>
								</nav>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
	</